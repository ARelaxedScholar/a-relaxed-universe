---
{"dg-publish":true,"permalink":"/1-cosmos/universal-language-model-fine-tuning-ulm-fit/","created":"2025-01-22T11:17:14.191-05:00","updated":"2024-05-20T21:44:33.049-04:00"}
---

202405032345
Status: #idea
Tags: [[1. Cosmos/Natural Language Processing (NLP)\|Natural Language Processing (NLP)]]
# Universal Language Model Fine-tuning (ULMFit
This is brilliant technique that consists in fine-tuning a pretrained language model say [[BERT\|BERT]] on a dataset related to the task that you want to make it do, before actually training it for the task. 

As a result the model is able to see more data, and the results are significantly better.
## References
[[2. White Holes/References/Practical Deep Learning for Coders#Chapter 10\|Practical Deep Learning for Coders#Chapter 10]]


