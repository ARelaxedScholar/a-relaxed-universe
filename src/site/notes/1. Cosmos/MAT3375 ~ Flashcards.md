---
{"dg-publish":true,"permalink":"/1-cosmos/mat-3375-flashcards/"}
---

TARGET DECK: Scholarly::University Notes::MAT3375
FILE TAGS: ai_generated, mat3375

-----
START
Cloze
The least squares estimate of the {{c1::slope}} in simple linear regression is {{c2::$b_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463485828-->
END

START
Cloze
The least squares estimate of the {{c1::intercept}} in simple linear regression is {{c2::$b_0 = \bar{y} - b_1 \bar{x}$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463486046-->
END

START
Cloze
The formula for the {{c1::residual sum of squares (RSS)}} in linear regression is {{c2::$\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463486183-->
END

START
Cloze
The {{c1::coefficient of determination}} $R^2$ is calculated as {{c2::$R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}$}}, where $\text{TSS}$ is the {{c3::total sum of squares}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463486334-->
END

START
Cloze
The total sum of squares (TSS) is given by {{c1::$\text{TSS} = \sum_{i=1}^{n} (y_i - \bar{y})^2$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463486470-->
END

START
Cloze
The standard error of the estimate for the {{c1::slope}} in linear regression is {{c2::$\text{SE}(b_1) = \sqrt{\frac{\text{RSS}}{(n-2)\sum_{i=1}^{n} (x_i - \bar{x})^2}}$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463486611-->
END

START
Cloze
The standard error of the estimate for the {{c1::intercept}} in linear regression is {{c2::$\text{SE}(b_0) = \sqrt{\frac{\text{RSS}}{n(n-2)} \left( \sum_{i=1}^{n} x_i^2 \right)}$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463486766-->
END

START
Cloze
A confidence interval for the slope $b_1$ in simple linear regression is given by {{c1::$b_1 \pm t_{\alpha/2, n-2} \cdot \text{SE}(b_1)$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463486921-->
END

START
Cloze
A confidence interval for the intercept $b_0$ in simple linear regression is given by {{c1::$b_0 \pm t_{\alpha/2, n-2} \cdot \text{SE}(b_0)$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463487061-->
END

START
Cloze
The null hypothesis for testing the slope in simple linear regression is {{c1::$H_0: \beta_1 = 0$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463487197-->
END

START
Cloze
The alternative hypothesis for testing the slope in simple linear regression is {{c1::$H_a: \beta_1 \neq 0$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463487336-->
END

START
Cloze
The test statistic for the slope in simple linear regression is {{c1::$t = \frac{b_1}{\text{SE}(b_1)}$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463487477-->
END

START
Cloze
The $F$-statistic in an ANOVA table for regression is calculated as {{c1::$F = \frac{\text{MSR}}{\text{MSE}}$}}, where $\text{MSR}$ is the {{c2::mean square regression}} and $\text{MSE}$ is the {{c3::mean square error}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463487614-->
END

START
Cloze
The mean square regression (MSR) is given by {{c1::$\text{MSR} = \frac{\text{SSR}}{p}$}}, where $\text{SSR}$ is the {{c2::sum of squares for regression}} and $p$ is the {{c3::number of predictors}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463487747-->
END

START
Cloze
The mean square error (MSE) is given by {{c1::$\text{MSE} = \frac{\text{RSS}}{n-p-1}$}}, where $n$ is the {{c2::sample size}} and $p$ is the {{c3::number of predictors}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463487890-->
END

START
Cloze
The assumption of {{c1::homoscedasticity}} in regression analysis means that the {{c2::variance of the error terms}} is constant across all levels of the independent variable(s).
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463488039-->
END

START
Cloze
{{c1::Multicollinearity}} in regression analysis refers to a situation where {{c2::two or more predictor variables are highly correlated}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463488178-->
END

START
Cloze
The {{c1::Variance Inflation Factor (VIF)}} is used to detect multicollinearity and is calculated as {{c2::$\text{VIF} = \frac{1}{1-R^2}$}}, where $R^2$ is the {{c3::coefficient of determination for the regression}} of one predictor on the other predictors.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463488308-->
END

START
Cloze
The {{c1::method of least squares}} aims to minimize the sum of the squares of the {{c2::differences between observed and predicted values}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463488468-->
END

START
Cloze
The {{c1::residuals}} in a regression model are defined as the {{c2::differences between the observed values and the predicted values}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463488595-->
END

START
Cloze
The {{c1::hat matrix}} in regression is given by {{c2::$H = X (X'X)^{-1} X'$}} and is used to project the {{c3::observed values onto the fitted values}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463488791-->
END

START
Cloze
The properties of the hat matrix $H$ include being {{c1::symmetric}} and {{c2::idempotent}}, meaning that {{c3:: $H^2 = H$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463488940-->
END

START
Cloze
In regression diagnostics, {{c1::Cook's Distance}} measures the influence of a {{c2::single observation}} on the {{c3::estimated regression coefficients}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463489126-->
END

START
Cloze
The {{c1::Durbin-Watson statistic}} is used to detect {{c2::autocorrelation}} in the residuals from a regression analysis.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463489270-->
END

START
Cloze
The {{c1::extra sum of squares principle}} involves comparing the {{c2::error sum of squares}} of a {{c3::full model and a reduced model}} to test the significance of additional predictors.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463489408-->
END

START
Cloze
One key assumption of linear regression is {{c1::linearity}}, which means the relationship between the predictor(s) and the outcome is {{c2::linear}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463489553-->
END

START
Cloze
Another assumption of linear regression is {{c1::independence}}, meaning the observations are {{c2::independent}} of each other.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463489698-->
END

START
Cloze
{{c1::Homoscedasticity}} means the variance of the error terms is {{c2::constant}} across all levels of the independent variable(s).
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463489761-->
END

START
Cloze
{{c1::Normality}} of the error terms means that the residuals should be {{c2::normally distributed}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463489933-->
END

START
Cloze
Residual plots are used to check the assumptions of {{c1::homoscedasticity}} and {{c2::linearity}} in a regression model.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463490316-->
END

START
Cloze
The {{c1::Durbin-Watson statistic}} is used to detect {{c2::autocorrelation}} in the residuals from a regression analysis.
Back Extra:
Tags: mat3375, ai-generated
END

START
Cloze
A high {{c1::Variance Inflation Factor (VIF)}} indicates {{c2::multicollinearity}} among the predictor variables.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463490455-->
END

START
Cloze
{{c1::Standardized residuals}} are used to identify {{c2::outliers}} in regression analysis.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463490603-->
END

START
Cloze
To handle categorical variables in regression, you use {{c1::dummy variables}}, which take values of {{c2::0 and 1}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463490762-->
END

START
Cloze
{{c1::Leverage}} is a measure of how far an {{c2::independent variable}} deviates from its mean.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463490917-->
END

START
Cloze
{{c1::Heteroscedasticity}} refers to the condition when the {{c2::variance}} of the error terms is {{c3::not constant}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463491054-->
END

# Hat Matrix
START
Cloze
The {{c1::hat matrix}} in regression is given by {{c2::$H = X (X'X)^{-1} X'$}} and is used to project the {{c3::observed values onto the fitted values}}.
Back Extra:
Tags: mat3375, ai-generated
END

START
Cloze
The properties of the hat matrix $H$ include being {{c1::symmetric}} and {{c2::idempotent}}, meaning that {{c3:: $H^2 = H$}}.
Back Extra:
Tags: mat3375, ai-generated
END

START
Cloze
The hat matrix $H$ is also known as the {{c1::projection matrix}} because it projects the {{c2::observed values}} onto the {{c3::fitted values}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463491215-->
END

START
Cloze
The residual maker matrix is given by {{c1::$M = I - H$}} and is used to compute the {{c2::residuals}} in regression analysis.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463491340-->
END

START
Cloze
The properties of the residual maker matrix $M$ include being {{c1::symmetric}} and {{c2::idempotent}}, meaning that {{c3:: $M^2 = M$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463491478-->
END

START
Cloze
The hat matrix $H$ and the residual maker matrix $M$ are related by the equation {{c1::$M = I - H$}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463491632-->
END

START
Cloze
The trace of the hat matrix $H$ is equal to the number of {{c1::parameters}} in the regression model, including the {{c2::intercept}}.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463491770-->
END

START
Cloze
The eigenvalues of the hat matrix $H$ are either {{c1::0}} or {{c2::1}}, reflecting its {{c3::idempotent}} property.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463491909-->
END

START
Cloze
The rank of the hat matrix $H$ is equal to the number of {{c1::independent variables}} in the regression model plus {{c2::one}} for the intercept.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463492056-->
END

START
Cloze
In the context of linear regression, the hat matrix $H$ is used to calculate {{c1::leverages}}, which measure the {{c2::influence}} of each observation on the fitted values.
Back Extra:
Tags: mat3375, ai-generated
<!--ID: 1718463492187-->
END
